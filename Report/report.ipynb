{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Hacettepe University Computer Science Department</font>\n",
    "## <font color=blue>Assignment 1</font>\n",
    "\n",
    "\n",
    "Name / Surname  : _Girayhan YILDIRIM_<br>\n",
    "Number          : _21527596_ <br>\n",
    "Course          : _BBM 409_  <br>\n",
    "Advisors        : _Dr. Aykut ERDEM, T.A. Necva BÖLÜCÜ_ <br>\n",
    "Due             : _06-11-2018_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: Theory Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- The difference between **linear regression** and **logistic regression** is that linear regression is used to predict a _**continuous value**_ while logistic regression is used to predict a _**discrete value**_.  In brief, linear regression is used for regression while logistic regression is used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Logistic Regression makes a prediction for the probability using a direct functional form where as Naive Bayes figures out how the data was generated given the results. Naive Bayes also assumes that the features are conditionally independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- \n",
    "* _False_\n",
    "* **True**\n",
    "* _False_\n",
    "* **True**\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-\n",
    "**Hidden Layer**\n",
    "\n",
    "* Two or fewer hidden layers will often suffice with simple data sets. However, with complex datasets involving time-series or computer vision, additional layers can be helpful.\n",
    "\n",
    "**Nodes in a hidden layer**\n",
    "\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Classification of Flowers using Neural Network\n",
    "## 1-Single Layer Neural Network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all , I've brought photos to the desired dimensions with the code file you provided.(code.py)\n",
    "#### Then I normalized these photo matrices using the following function.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __normalize(self,value):\n",
    "        return value / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I created normalized and batching train data , initial weights and bias vectors as objects in singleLayer class.This is batch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __batch_file(self):\n",
    "        batch_of_input = []\n",
    "        batch_of_label=[]\n",
    "        for b in np.arange(0, len(self.X), self.batch_size):\n",
    "            try:\n",
    "                batch_of_input.append(self.X[b:b + self.batch_size])\n",
    "                batch_of_label.append(self.labels[b:b + self.batch_size])\n",
    "            except:\n",
    "                batch_of_input.append(self.X[b:])\n",
    "                batch_of_label.append(self.labels[b:])\n",
    "\n",
    "        return np.asarray(batch_of_input),np.asarray(batch_of_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is my class constructor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class singleLayer:\n",
    "    def __init__(self,train_data,train_data_labels,epoch_num,learning_rate,batch_size):\n",
    "        self.X = self.__normalize(train_data)\n",
    "        self.labels =self.__modify_labels(train_data_labels)\n",
    "        self.W = np.random.rand(self.X.shape[1] , 5)/100\n",
    "        self.bias = np.zeros((1,5))\n",
    "        self.epoch = epoch_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My train parameters are epoch number , learning rate and batch size.\n",
    "#### My loss function correct label of negative log likehood in each batch.That function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __loss(self,probs,bl,size):\n",
    "        num_examples=size\n",
    "        correct_logprobs = -np.log(probs[range(num_examples), bl])\n",
    "        cross_entropy = np.sum(correct_logprobs) / size\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict output is `xT . w + b` and call `softmax` and `sigmoid` function.My softmax functions is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __softmax (self,w):\n",
    "        return np.exp(w) /np.sum(np.exp(w))\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And updating weights and bias for minimazed loss each iteration.My update function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __update_weight(self,X,score):\n",
    "        dW = np.dot(X.T,score)\n",
    "        db = np.sum(score, axis=0, keepdims=True)\n",
    "        self.W += self.learning_rate*dW\n",
    "        self.bias += self.learning_rate*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This graph is change loss value each iteration:\n",
    "<img src=\"single.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My visulazition code is 1 x 768 model size convert to 32 x 24 dimension and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model):\n",
    "    classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
    "    for i in range(5):\n",
    "        img = model.T[i][0:768]\n",
    "        img = np.reshape(img, (32, 24))\n",
    "        plt.suptitle(classes[i])\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My model visulazition model output :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ddr.png\"><img src=\"st.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Multi Layer Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My neural network architecture is Layer object list `neural_network =[input_layer,hidden_layers ,output_layer ]`.All objects input, output , weights etc. have a so many features.<br>\n",
    "<img src=\"layergraph.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I determined batching size in multiLayer class and batching input data by batching size.Create neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiLayer:\n",
    "    def __init__(self,train_data,train_data_labels,epoch_num,learning_rate,batch_size,hidden_layer_list):\n",
    "        self.X = self.__normalize(train_data)\n",
    "        self.labels =self.__modify_labels(train_data_labels)\n",
    "        self.label=list(train_data_labels[0])\n",
    "        self.W = np.random.rand(self.X.shape[1] , 5)/1000\n",
    "        self.epoch = epoch_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size                #####     determine batch size\n",
    "        self.loss_list = []\n",
    "        self.neural_network=[]\n",
    "        self.hidden_layer_list=hidden_layer_list\n",
    "\n",
    "############# creating neural network architecture  ############################\n",
    "        \n",
    "        self.neural_network.append(self.create_input_layer(self.hidden_layer_list[0]))\n",
    "        for i in range(0,len(self.hidden_layer_list)-1):\n",
    "            self.neural_network.append(self.create_hidden_layer(self.hidden_layer_list[i],self.hidden_layer_list[i+1]))\n",
    "        self.neural_network.append(self.create_hidden_layer(self.hidden_layer_list[-1], 5))\n",
    "        self.neural_network.append(self.create_output_layer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I create objects in other classes through functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-21-b5a753e8031a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-b5a753e8031a>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def create_input_layer(self, next_layer_neuron_size):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "######      CREATİNG OBJECT FUNCTİONS           #############\n",
    "    def create_input_layer(self, next_layer_neuron_size):\n",
    "        input_layer = inputLayer.inputLayer(self.X.shape[1], next_layer_neuron_size)\n",
    "        return input_layer\n",
    "\n",
    "    def create_hidden_layer(self,neuron_size,next_layer_neuron_size):\n",
    "        hidden_layer = hiddenLayer.hiddenLayer(neuron_size, next_layer_neuron_size)\n",
    "        return hidden_layer\n",
    "\n",
    "    def create_output_layer(self):\n",
    "        output_layer = outputLayer.outputLayer(5)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have identified the volume in your hidden layer through a list.List length is a hidden layersize.Each layer neuron size is each element of list.\n",
    "#### For example `hidden_size_list = [6,3,4]` mean we have a three hidden layer and first hidden layer has a six neuron , second hidden layer has a three neuron and third hidden layer has a four neuron.Then I installed this structre between input and output layer.View that structure:\n",
    "<img src=\"634.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And I've already shown the batch function I'm calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I determine hidden layer size list . After the create multiLayer object.(neural network object list create in multiLayer class) I call the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'multiLayer' has no attribute 'multiLayer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d9e084911b1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhiddensizelist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#   parameters (train data input, train data label , epoch num , learning rate , batch size , hidden layer arc.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmulti_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhiddensizelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmulti_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'multiLayer' has no attribute 'multiLayer'"
     ]
    }
   ],
   "source": [
    "hiddensizelist=[6,3,4]\n",
    "#   parameters (train data input, train data label , epoch num , learning rate , batch size , hidden layer arc.)\n",
    "multi_layer = multiLayer.multiLayer(train_data_input,train_data_label,1000,0.02,30,hiddensizelist)\n",
    "multi_layer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I use forward and backward algorithm when training.Backward is from begin to finish input weight production and activation function call also prime shape output.Backward is from finish to begin and minimazed error and update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self):\n",
    "    for i in np.arange(0,len(self.neural_network)-1):\n",
    "        self.neural_network[i+1].z = np.dot(self.neural_network[i].a,self.neural_network[i].weight) # multipication\n",
    "        self.neural_network[i+1].a = self.__sigmoid(self.neural_network[i+1].z)                     # activation function\n",
    "        self.neural_network[i+1].prime = self.__sigmoidPrime(self.neural_network[i+1].a)            # holding prime value\n",
    "    \n",
    "    \n",
    "def backward(self):\n",
    "    # all node update and calculate without first node\n",
    "    for i in np.arange((len(self.neural_network)-1) , 1 , -1):\n",
    "        self.neural_network[i].delta = (self.neural_network[i].error * self.neural_network[i].prime)\n",
    "        self.neural_network[i-1].error = np.dot(self.neural_network[i].delta,self.neural_network[i-1].weight.T)     # 30 x 5  . 5 x 4    30 x 4\n",
    "        self.neural_network[i-1].weight += self.learning_rate * np.dot(self.neural_network[i-1].z.T,self.neural_network[i].delta) # 30 x 4 30 x 5\n",
    "    \n",
    "    # first node update and calculate\n",
    "    self.neural_network[1].delta = (self.neural_network[1].error * self.neural_network[1].prime)\n",
    "    self.neural_network[0].weight += self.learning_rate * np.dot(self.neural_network[0].a.T,self.neural_network[1].delta)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I used the different activation function.It is necessary derivatives that function.I determine its."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __sigmoid(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "\n",
    "    def __sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def __tan(self, x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def __tan_prime(self, a):\n",
    "        return 1 - np.square(a)\n",
    "\n",
    "    def __reLU(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def __reLU_prime(self, x):\n",
    "        x[x <= 0] = 0\n",
    "        x[x > 0] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have a two error calculate function.Cross entropy and sum of square.Thats function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   output is a last element of architecture and sum of square\n",
    "def __loss(self,true):\n",
    "    out_layer = self.neural_network[-1]\n",
    "    out_layer.error = (true - out_layer.a) * self.__sigmoidPrime(out_layer.a)\n",
    "    return np.sum(self.neural_network[-1].error ** 2)\n",
    "    \n",
    "    \n",
    "def __cross_entropy(self,last_element,bl,size):\n",
    "    num_examples=size\n",
    "    correct_logprobs = -np.log(last_element[range(num_examples), bl])\n",
    "    cross_entropy = np.sum(correct_logprobs) / size\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIFFERENT PARAMETERS AND SAMPLE OUTPUT HIDDEN LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch=100 , learning rate = 0,02 , batch size = 32\n",
    "<img src=\"100epochhidden.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch=1000 , learning rate = 0,02 , batch size = 32\n",
    "<img src=\"1000epochhidden.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch=1000 , learning rate = 0,005 , batch size = 32\n",
    "<img src=\"1000epochhiddenlow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch=100 , learning rate = 0,02 , batch size = 64\n",
    "<img src=\"10064.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch=1000 , learning rate = 0,02 , batch size = 64\n",
    "<img src=\"100064.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch=1000 , learning rate = 0,005 , batch size = 64\n",
    "<img src=\"100064low.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESOURCES\n",
    "https://medium.com/@michaeldelsole/a-single-layer-artificial-neural-network-in-20-lines-of-python-ae34b47e5fef\n",
    "\n",
    "https://enlight.nyc/projects/neural-network/\n",
    "\n",
    "https://medium.com/analytics-vidhya/neural-networks-for-digits-recognition-e11d9dff00d5\n",
    "\n",
    "http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "\n",
    "https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py\n",
    "\n",
    "https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
